{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03d48c6c-fdaf-4efc-a584-566f90296525",
   "metadata": {},
   "source": [
    "<H1> Heart Disease prediction for early detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fb34b4-4d32-4bf4-8088-a80f9530a975",
   "metadata": {},
   "source": [
    "<H2>Data Loading and Environment Setup</H2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08fb5d5-97c8-4455-b84c-1eab8ba259b9",
   "metadata": {},
   "source": [
    "Link- https://www.kaggle.com/datasets/neurocipher/heartdisease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c77607c-7192-4cb8-bd7b-26e1d6c1acb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import kagglehub\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e0f68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"neurocipher/heartdisease\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cb9f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = '/Users/abrarzarif/.cache/kagglehub/datasets/neurocipher/heartdisease/versions/1'\n",
    "\n",
    "print(os.listdir(path))\n",
    "csv_file = os.path.join(path, 'Heart_Disease_Prediction.csv')  # adjust filename if different\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca333fdd-19fe-46fe-a268-34da2cb7c585",
   "metadata": {},
   "source": [
    "<H2>EDA</H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcf8c8b-38a0-4355-8839-7954bc9b15a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cdc93a-69a9-45ff-aa32-c83ed44476d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501bcad8-b6e6-45ab-9f0d-dd3f883cb864",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()  #info finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29830fb7-83f8-40eb-bbf7-0325851e4eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum() #Finding missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2551aae7-5ffc-44c9-b60e-da7ee5823350",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()  #statistics finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f11f782-f464-4464-b283-655580252338",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()  #Finding if there is any duplicated value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7235d5cf-af23-4df8-8ea7-712888ba747d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Heart Disease\"] = df[\"Heart Disease\"].replace({\n",
    "    \"Presence\": 1,\n",
    "    \"Absence\":0\n",
    "})                   #Encoding heart disease catagorical data with numerical data for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619804e0-7aa3-4ba0-b96d-0c429a67bbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e6d0db-787f-4f90-80a4-09eef0f2a5c5",
   "metadata": {},
   "source": [
    "<H2>Univariate analysis</H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf07048f-e804-447e-90b3-04697feae8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Heart Disease'].value_counts().plot(kind='pie',autopct=\"%2f\") #Finding percentage of people having/not having heart diseases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1db46d-6f69-4076-9e3b-8303eef2f7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#not imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa66c7af-9314-4e4d-a4d6-a314a61333d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.subplot(1,4,1)\n",
    "sns.boxplot(x=df['BP']) #boxplots for outlier detection\n",
    "plt.subplot(1,4,2)\n",
    "sns.boxplot(x=df[\"Age\"])\n",
    "plt.subplot(1,4,3)\n",
    "sns.boxplot(x=df['Max HR'])\n",
    "plt.subplot(1,4,4)\n",
    "sns.boxplot(x=df['Cholesterol'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33813cf8-1f5d-4a0d-a164-f0f696dc1a8e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "#There are outliers, need to delete \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f9f4a9-f5ae-43a5-b143-e1d422259ed6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<H2>Multivariate Analysis</H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8299aac-8576-4e77-ac14-c4179fea5189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "features = ['Sex', 'Chest pain type', 'BP', 'Cholesterol', 'FBS over 120', \n",
    "            'EKG results', 'Max HR', 'Exercise angina', 'ST depression', \n",
    "            'Slope of ST', 'Number of vessels fluro', 'Thallium', 'Age']\n",
    "\n",
    "for feature in features:\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Histogram\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.hist(df[feature], bins=20, color='black', edgecolor='black', alpha=0.5)\n",
    "    plt.title(f'{feature} Distribution', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    sns.histplot(data=df, x=feature, hue='Heart Disease', \n",
    "                 palette=['#2ecc71', '#e74c3c'], alpha=0.6, kde=True)\n",
    "    plt.title(f'{feature} vs Heart Disease', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Scatterplot \n",
    "    plt.subplot(1, 3, 3)\n",
    "    sns.scatterplot(data=df, x='Age', y=feature, hue='Heart Disease', \n",
    "                    palette=['#2ecc71', '#e74c3c'], alpha=0.8)\n",
    "    plt.title(f'Age vs {feature}', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c926feb6-be71-4701-b0f5-289774c6f57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Important features identified- chest pain type, bp, cholestorol, max HR, angina, Ischemia, Number of vessel fluro, Thallium, Age\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ee98ad-529d-43b3-8ff2-d1d6be969176",
   "metadata": {},
   "source": [
    "<H2> Feature Engineering </H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daf64d5-3ed3-4288-ab2e-617df6554231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ischemia_risk(row):\n",
    "    if row[\"ST depression\"] >= 2.0 and row[\"Slope of ST\"] == 3:\n",
    "        return \"high\"                                           #Engineering a new feature called ischemia_risk by combining ST depression and slope of ST\n",
    "    elif row[\"ST depression\"] >= 1.0:\n",
    "        return \"moderate\"\n",
    "    else:\n",
    "        return \"low\"\n",
    "\n",
    "df[\"ischemia_risk\"] = df.apply(ischemia_risk, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4971a3bd-ccf1-4495-a368-40d39c661508",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=[\"ischemia_risk\"], drop_first=False)\n",
    "df[\"ischemia_risk_low\"] = df[\"ischemia_risk_low\"].astype(int)\n",
    "df[\"ischemia_risk_moderate\"] = df[\"ischemia_risk_moderate\"].astype(int) #one hot encoding the ischemia risks \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88321af5-0362-453c-9e4b-d8af6d0ae83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"ischemia_risk_high\", \"EKG results\", \"FBS over 120\"], axis=1) #Dropping less important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8429131b-dac2-4e33-a355-410d186b65cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"ST depression\", \"Slope of ST\"], axis=1) #dropping features which are already combined into another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1916707-aa95-405e-b328-37fcdceb4240",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-test split before scaling\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "df_train, df_test = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True )\n",
    "\n",
    "#splitting target from training\n",
    "X_train = df_train.drop(columns=[\"Heart Disease\"])\n",
    "y_train = df_train[\"Heart Disease\"]\n",
    "\n",
    "X_test = df_test.drop(columns=[\"Heart Disease\"])\n",
    "y_test = df_test[\"Heart Disease\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923d63aa-83a5-4bc0-a145-12f2cef8404f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0a5d47-9e2c-40aa-84cb-51ac240d3999",
   "metadata": {},
   "source": [
    "\n",
    "<H2>Model Implementation</H2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b64500-d38b-423d-865d-24318cdef8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression         #Binary classifier problem so choosing Logistic Regression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Columns with outliers\n",
    "outlier_cols = [\"Cholesterol\", \"BP\", \"Max HR\"]\n",
    "normal_cols = [c for c in X_train.columns if c not in outlier_cols]\n",
    "\n",
    "# Preprocessing: Robust scaling for outliers, standard scaling for others\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"robust\", RobustScaler(), outlier_cols),\n",
    "    (\"standard\", StandardScaler(), normal_cols)\n",
    "])\n",
    "\n",
    "reg = Pipeline([\n",
    "    (\"preprocess\", preprocessor),         #using Elasticnet as dataset is less than 50k samples, but not extremely small. \n",
    "    (\"model\", LogisticRegression(\n",
    "        penalty=\"elasticnet\",\n",
    "        solver=\"saga\",     \n",
    "        l1_ratio=0.7,        \n",
    "        C=10,               \n",
    "        max_iter=5000,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "# Train\n",
    "prd=reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_lr = reg.predict(X_test)\n",
    "y_prob_lr = reg.predict_proba(X_test)[:, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fb765e-c40d-4f71-8614-b310d51b4e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#will check here if robust scaling actually worked in reducing the outliers\n",
    "# Get fitted preprocessor\n",
    "preprocessor_fitted = reg.named_steps[\"preprocess\"]\n",
    "\n",
    "# Transform training data\n",
    "X_train_transformed = preprocessor_fitted.transform(X_train)\n",
    "\n",
    "# Get column order after ColumnTransformer\n",
    "robust_features = outlier_cols\n",
    "standard_features = normal_cols\n",
    "all_features = robust_features + standard_features\n",
    "\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    X_train_transformed,\n",
    "    columns=all_features,\n",
    "    index=X_train.index\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549b0487-fc50-4d87-b7af-f7591f58b6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = \"Cholesterol\"\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Before scaling\n",
    "axes[0].boxplot(X_train[feature], vert=False)\n",
    "axes[0].set_title(f\"Before Scaling: {feature}\")\n",
    "\n",
    "# After Robust Scaling\n",
    "axes[1].boxplot(X_train_scaled[feature], vert=False)\n",
    "axes[1].set_title(f\"After Robust Scaling: {feature}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5abfd9e-214c-40c1-a2fc-f0b12e50eee9",
   "metadata": {},
   "source": [
    "<H2>Performance Metrics</H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adabcd30-fa9c-4489-938b-0bb8a44a798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_model(name, y_true, y_pred, y_prob):\n",
    "    print(f\"\\n{name}\")\n",
    "    print(\"-\" * len(name))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"ROC-AUC:\", roc_auc_score(y_true, y_prob))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc079c1-f13e-4f8e-8f42-32b233f7b07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(\"Logistic Regression\", y_test, y_pred_lr, y_prob_lr)\n",
    "accuracy = accuracy_score(y_test, y_pred_lr)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "recall = recall_score(y_test, y_pred_lr)\n",
    "print(f\"Recall:{recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b88195-f19e-4dbc-8c42-b56715989cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "cmlr = [[32, 1],\n",
    "      [5, 16]]\n",
    "\n",
    "labels = ['No Disease (0)', 'Disease (1)']\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cmlr, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - Logistic Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215f38af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the model for deployment\n",
    "import pickle\n",
    "with open(\"reg.pkl\",\"wb\") as f:\n",
    "    pickle.dump(reg, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d406b68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
